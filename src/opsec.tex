\documentclass[11pt]{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[total={7in,9in}]{geometry}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{hyperref}

\title{Succinct proofs and provable specifications for critical systems}
\author{ \\ Morgan Thomas \\ Casper Association \\ morgan@casper.network }


\begin{document}

\maketitle

\begin{abstract}
	It's not obvious how to know things about the world, but knowing that
	critical systems will work correctly is a matter of duty for the
	engineers who work on them. How can we get beyond uncertainty into a
	realm of being reasonably certain that the systems we build will work,
	or at least won't exhibit certain particularly unacceptable behaviors? 
	Is this even realistic? This proposal advises that it is realistic to
	obtain a reasonable degree of certainty of systems we build
	satisfying negative requirements we place on their behaviors, for
	a broad set of use cases which can be supported by succinct proving
	with distributed, decentralized verifying. With zero knowledge proving,
	these processes can also protect privacy. The Open Specification Language
	(OSL) and its compiler, which are designed to support succinct, zero knowledge proving
	and formal verification of the correctness of those succinct proving processes,
	can play a supportive role in building
	systems whose conformance to their negative behavioral requirements is verifiable
	at runtime and knowable before the fact.
\end{abstract}

What does it mean to know something, can we know anything, and if so, how
can we know things, while also knowing that we know them? Perhaps these questions
gain their greatest sense of urgency in the context of duty, such as when we are
responsible for people's lives, or for people's money. To know that a life-sustaining
piece of tech is working as intended is often the responsibility of engineering teams.
Is this duty even possible to fulfill?

Let's suppose for the sake of argument that we are responsible for designing a system
which takes as input a pair $(a,b)$  of billion-digit integers and outputs the sum $a + b$.
Let's suppose that the stakes are as high as they can be; if the system outputs the wrong
sum, then potentially people will die as a result. How can we know that the system is always
working as intended? The example is contrived, but since it is such a simple example,
if we can't solve this instance of the problem, then chances are we can't solve the problem
of knowing that a system is working as intended for any interesting application.
It turns out that a lot of difficult issues arise in trying to solve for this contrived example.

We can immediately throw out the option of hand-checking the sums every time,
because this process is too slow, too expensive, and too susceptible to human error.

Let's suppose we can test the system on millions of randomly generated inputs, using
some known-good process to find the sums $a + b$ of the random pairs $(a, b)$, and
comparing those expected sums against the actual sums output by our system. If so,
this will provide empirical evidence, but not overall certainty, that the system will always
work as intended. There are a few issues which limit the usefulness of this type of testing:

\begin{itemize}
	\item Often, the sample space is too large; there are too many cases to test all
		of them, and so we need to select a subset of cases to test, which leaves
		open the possibility that the system doesn't work on some untested inputs.
	\item In order to draw conclusions about future behavior from past tests, we need
		to assume that the system will behave in the future the same ways it behaved
		in the past under the same conditions.
	\item Conditions under which a system is tested may be different from conditions
		under which it is used, resulting in different behavior on the same inputs
		in testing versus production. For example, interference from actors with
		bad intentions may occur in production when it did not occur in testing.
	\item This type of testing presupposes that we have a known good process for checking
		a result or generating a correct result. If we don't have a known good process
		to check against, then the best we can get out of this type of testing is
		evidence that two different processes for arriving at the answer get the
		same answer. This means that if either process is wrong, then both processes
		are giving the same wrong answer, as far as the tested cases go.
\end{itemize}

Testing is a form of science. Science is the process of proposing falsifiable hypotheses and
then performing tests which either refute them or fail to refute them.
A test of a hypothesis must be such that it has multiple imaginable observable outcomes,
some of which would refute the hypothesis if they were observed.
Failing to refute a hypothesis provides some amount of evidence for the hypothesis, 
while leaving open the possibility that it is wrong.

It is debatable whether science ever yields knowledge about the future. It is however clear
that for the purposes of engineering, science is the only process which could potentially
yield knowledge about the future. Without using science we cannot know anything except
what we know \emph{a priori}, that is, prior to experience. \emph{A priori}\/ truths would
still be true regardless of what kind of world sense data presents. They include logical
and mathematical truths. \emph{A priori}\/ truths will not by themselves give us knowledge
that a system will work as intended, because a system is a thing that exists physically and
presents itself to us through sense data. There is nothing we can know \emph{a priori}\/
about the future behavior of a system.

\emph{A priori}\/ knowledge is useful for engineering, in combination with scientific
knowledge. This is true when and to the extent that some description of a mathematical object
can serve as a model of a real world system. Modeling is a useful technique because
\emph{a priori}\/ knowledge about mathematical objects is possible. To the extent
that something in the world resembles a certain mathematical model, we can know propositions
about the actual thing which are predicted by the hypothesis that the model resembles
the actual thing.

The following points are debatable: that we can have
\emph{a priori}\/ knowledge, and that we can derive scientific knowledge by the process
of modeling and testing. Indeed, there is still a debate as to whether we can have knowledge.
There is also a debate as to how knowledge can be defined. My goal here is not to get
into any of these debates. I cannot define knowledge or convince a hardcore skeptic
that knowledge is possible. Instead, I assume an audience that already believes in the
possibility of knowledge about the future behavior of systems, and I am mainly interested in
the technical question of how such knowledge can be obtained.

Part of the usual connotation of knowledge is truth. For example, Aristotle defines knowledge
as justified true belief. By this definition, everything known is true. The truth is the
juice (what we want from the knowledge), and the justification is the squeeze (the basis
on which we have the knowledge). Aristotle's definition of knowledge is not only
insufficient, as argued by Gettier (1963),\footnote{Gettier, E. L. (1963).
``Is Justified True Belief Knowledge?'' Analysis 23: 121-3.}
but also terribly vague and ambiguous as to what constitutes sufficient or
right justification for knowledge.

Let us try to disambiguate knowledge a bit, in a pragmatic way, by saying that to
have knowledge, there must be only a negligible probability that we would have the
justification we have and the known proposition would be false. This allows for the
possibility that there is a world in which we have the justification we have and the
proposition is false, but it requires the set of such worlds to be so unlikely to occur
that it is practically not significant that they are possible.

The notion of ``negligible'' is still ambiguous. What one person considers negligible
in one context may be non-negligible to another person or to the same person in another
context. At what point is a probability negligible if the possibility in question entails
human lives ending due to engineering failure? To answer this question rigorously, we may have
to put a dollar value on a human life, which maybe feels morally reprehensible but
is occasionally necessary in cost / benefit analysis. If it makes you feel better,
you can put a tremendous value on a human life, like a hundred billion dollars.
Then we can define a negligible possibility of death as one whose expected value
is a trivial negative amount of money,
say for instance a dollar. We obtain the expected value by multiplying the cost of the
event (a negative dollar amount) by the probability (a number between 0 and 1).

Some reflection should show that this allowance of a negligible probability
of our knowledge being false is necessary if we want to say that we can know things
about the actual world. If we want to say we know anything about the past then we have
to assume that our memory is to some extent accurate. If we want to say we know anything
about the future then we have to make some assumptions saying that the future will
be like the past in certain ways.

With all of that groundwork laid, I would like to suggest a general strategy for
knowing that a system's future behavior will be as intended. One must make some base
assumptions about the system's future behavior. Based on those assumptions, one uses
logic (i.e., truth-preserving inference) to prove that the system will behave as intended
if the base assumptions hold. The base assumptions must be such that there is only a
negligible probability of them not being true, or if there is a non-negligible probability
of them not being true, then the worst consequences that can result are acceptable.
This description takes into consideration that a system may have various failure modes,
some more acceptable than others. E.g., it is more acceptable if a plane's engines fail
to start on the runway, than if they stop unexpectedly while the plane is flying.

The strategy just outlined allows for obtaining knowledge with the highest obtainable
degrees of confidence. We can break down this strategy into four steps, not to be
performed in order but in an iterative process, interleaved in no particular order:
\begin{enumerate}
	\item selecting base assumptions which appropriately limit the risk entailed
		by the possibilities of those assumptions being false;
	\item testing those base assumptions extensively to get a sense of how
		likely it is that they will be false under actual conditions;
	\item ensuring that if the base assumptions are true, then necessarily the
		system will work as intended, because:
		\begin{enumerate}
			\item the intentions have been correctly expressed as propositions in a formal language;
			\item those propositions follow from the base assumptions by a series of truth-preserving inferences.
		\end{enumerate}
\end{enumerate}
The base assumptions are the things that we cannot know \emph{a priori}\/ but must
assume about the system and its operating conditions in order to know \emph{a priori}\/ that
the system would behave as intended under those conditions. The base assumptions can
also be conditions under which the system would not exhibit certain failure modes.
For example, a computer system cannot be expected to exhibit the correct behaviors when it has
no power supply, but it can still be expected not to exhibit arbitrary incorrect behaviors
when it has no power. So to know that the system does the right things, we must assume
a power source, but to know that the system does not do the wrong things, we need not assume
a power source, in this example.

Steps 3(a-b) amount to verifying that the base assumptions are sufficient conditions
for correct operation of the system as designed. With these steps completed, we know
\emph{a priori}\/ that the system would behave as designed if the base assumptions were
true. In other words, with steps 3(a-b) completed, we know that we have completely
expressed, as the base assumptions, what needs to be tested.

Let us now consider some approaches we could take to apply this strategy to building
and validating a system for adding together two billion-digit integers.

For example, let's say that we build an adding circuit out of transistors in a
semiconductor chip.\footnote{This is an oddball example; more typically we would solve this
problem by writing a computer program, but this example has the advantage of giving
us less to think about when we go to verify the solution, because we only have
to verify the hardware, not the hardware and the program, and the hardware should
have relatively simple behavior, since it should only add and nothing else.}
For our base assumptions, we assume that we can model the chip with a circuit diagram,
relying on the transistors exhibiting some standard, specified transistor behaviors.
In other words, we assume that the chip will behave the same way that its circuit diagram
behaves, in some idealized mathematical interpretation of the diagram's behavior.
In this way we can think of the circuit as computing a mathematical function (from
the input pin states to the output pin states), the same function specified by its
circuit diagram. We can prove \emph{a priori}\/ that the function specified by the
circuit diagram is the intended addition function, and we can test that
the circuit behaves the same as its circuit diagram.

A known issue with this approach is that semiconductor chips do not exhibit
their circuit diagrams' mathematically ideal behavior under all conditions. For
example, radiation in the environment can cause a circuit to stop exhibiting
its intended behavior and instead exhibit random or semi-arbitrary behaviors.
Also, for example, a circuit's behavior may deviate from the norm outside of its
stable operating temperature range. We can rely on circuits to behave as intended
almost all of the time, but if this is not good enough, if we need to get the
right answer every time, then the approach needs some more work.

We can for example try having two copies of the same circuit run in parallel,
both on the same input. If they give different answers, then we keep asking
them the question until they give the same answer. This approach lowers the
odds that the system will give the wrong answer. We can keep repeating the
computation as many times as we want until the probability is negligible that
we got the wrong answer, unless somehow all of the redundant computations
were tampered with to produce the same wrong answer.

What if this is still not good enough? What if we want to ensure that in the event
where we are unable to perform a correct computation, because our computing
instruments have been tampered with somehow and always give the wrong result,
then our system still will not give the wrong answer, and will instead fail
to give an answer? Can this be done?

In extremis, probably this can't be done. Let's suppose that God decides one day
to modify the laws of physics such that all of the adding circuits now give
the result that $2+2=5$ where they used to give the result $2+2=4$. Can we have
a trusted computing environment if God is interested in changing the laws of physics
up on us just to try to trick us into having false beliefs about numbers? I would say
probably not.

Let's try to set our sights a little lower, on something that could potentially be done.
Let's say that we want to verify that a computation was done correctly, without
assuming that any particular execution environment hasn't been tampered with. Let's also
not assume that any particular small set of execution environments contains one which
hasn't been tampered with. But let's assume that we can produce an execution environment
which we can trust, say by buying one new at the store, or by ordering one to be made
from scratch. Let's assume that God won't change the laws of physics under us, so that
circuits will continue to behave the same way tomorrow. Can we then solve a problem,
such as adding two integers, and know that our answers are correct under such assumptions,
that is, without assuming that any particular execution environment is working right,
or assuming that any particular small set of execution environments contains one that is
working right?

In theory, it can be done. One can imagine a process where before we accept the answer,
we randomly select a million computers out of all the computers that exist, and we
also have a million randomly selected people independently compute the answer by hand with pencil and
paper, and then we accept as true only an answer validated by most of the parties
which we ourselves computed to be true. The idea here would be that each person who
can be trusted can trust themselves, and if my answer is what 99\% of people and computers
in the large sample got, then odds are I didn't make an error and they're not all lying to me. Of course,
the issue with this approach is that it's impractical; we can't actually deploy a solution
to anything that works this way.

This is where succinct proof systems start to look really useful for solving the problem.
Succinct proofs allow us to gain knowledge by doing a fairly small, bounded amount of work
as long as the proposition we want to know is stated concisely.
For example, one can use a succinct proof to show ``$h(x)$, $h(y)$, and $h(z)$ are
respectively SHA-256 Merkle hashes of some billion-digit integers $x$ and $y$ and some
integer $z$ such that $x + y = z$.'' In this statement, $h(x), h(y)$ and $h(z)$ each
refer to specific numbers, whereas $x, y,$ and $z$ are variables of unspecified values.
Whereas $x$ and $y$ each have a billion digits, their hashes are short, and therefore
the statement as a whole is short.

Let's suppose we are given $x$ and $y$, and we are going to find $z = x + y$ and check
our work by the following approach. We first compute $z$, $h(x)$, $h(y)$, and $h(z)$.
We don't have to be extraordinarily careful in doing so, because we will prove and verify
that it was done correctly, in the remainder of the process. Next, we generate
a succinct proof $P$ that $h(x), h(y),$ and $h(z)$ are respectively the SHA-256
hashes of some billion-digit integers $x$ and $y$ and some integer $z$ such that
$x + y = z$. Next, we distribute the tuple $(h(x), h(y), h(z), P)$ into a cluster of
people's devices, say a million devices with a million owners in a decentralized
network, and each of these devices checks that $P$ is a proof of the statement.
Based on the results, we conclude either that all honest verifiers verified the proof,
or that perhaps something else is the case.

In addition, we check that we computed the hashes correctly, using our distributed 
network of verifiers. We do this by constructing each hash as the root of a Merkle
hash tree over the sequence of digits in the input number, and randomly ask verifiers
in our network to check our computations of nodes in the Merkle hash trees, just
to convince ourselves that our computer was not tampered with in such a way that it
computed the hashes wrong.

What, if anything, have we gained by operating such a way? Do we finally know that
$x + y = z$ if we can complete such a process? Let's consider a happy path scenario,
where all verifiers successfully verified the proof and the Merkle hash trees.
Given that succinct proof systems are probabilistic proof systems, there remains at
least a negligible probability that the statement being proven is not true and all
the verifiers accept the proof. Also,
there remains at least a negligible probability of a false positive due to hash collision,
that is, that $x + y \neq z$ but someone found (other) numbers $x', y', z'$ such
that $x' + y' = z'$, $h(x) = h(x')$, $h(y) = h(y'),$ and $h(z) = h(z')$. In such a
case of hash collisions, the statement $P$ proves is true but it does not show the
thing we ultimately want to show, that $x + y = z$. In conclusion, there remains at least a negligible
probability that even if we see all the verifiers verify the proof, that nonetheless
$x + y \neq z$.

There could be a lot to recommend this kind of approach, depending on
what you believe about it and what you are looking for. Computing the numbers $z, h(x), h(y)$,
and $h(z)$ will be computationally hard, and computing the proof $P$ will also be hard.
Transmitting $x, y,$ and $z$ is relatively expensive in terms of bandwidth / time,
but transmitting $h(x), h(y), h(z)$ and $P$ is relatively cheap. Verifying $P$ is
computationally easy, relatively speaking. By verifying $P$ over and over in a sufficiently
distributed, decentralized, and perhaps diverse network of verifier devices, we may gain
sufficient confidence that $P$ was checked by some verifier device which was not tampered
with and which behaved correctly. Similarly, by distributing the process of verifying
the Merkle hash trees across the network, we may gain sufficient confidence that the
input numbers were hashed correctly. Usefully, this can all happen without transmitting
$x, y,$ and $z$ in their entirety to any of the verifier devices, and without any
of the verifier devices computing $x + y$. By following this approach, we potentially get
a probability of knowing $x + y = z$ and it being true which is like the probability we
would get by redundantly computing $x + y$ many, many times in a decentralized, distributed,
and diverse network of devices, but at a fraction of the cost of doing so.

In some applications, there is sensitive, private information involved. If we want to
use a massive distributed network to verify facts based on such information, it may
be problematic from a privacy standpoint. Fortunately, succinct proving systems are also
often zero-knowledge proving systems, which means roughly that the proofs do not reveal the
information used to generate the proofs which is not contained in the statements they prove.
This zero-knowledge characteristic of a succinct proving system means that we can use it
to verify publicly available statements using publicly available proofs, even when the process
of generating the proofs required inputs which were private.

Actually executing on such an approach is a many-faceted problem. It involves building
a distributed, decentralized, and diverse network of verifier devices. Each device is probably a
combination of hardware and software. The hardware need not be entirely bug-free, as long as it
is diverse enough that not all verifier devices will fail in the same way due to a hardware bug.
If the hardware is diverse, then there will be more than one verifier program, although they
might all be compiled from the same source code. If all of the verifier programs in the network
are compiled from the same source program, then it is probably good enough if that source
program is proven secure, meaning that (in the semantics of the source language) it will not
accept a proof unless that proof satisfies the rules of the succinct proof protocol.

Another interesting approach to building a network of verifier devices could be to
make an ASIC or an FPGA program which contains all of the logic for
performing the verifier protocol and which is proven (in the semantics of the circuit diagram
language) to be secure in the sense of always rejecting invalid proofs. This would
probably be more expensive than a software solution, but it would remove some sources
of uncertainty, such as around the existence of bugs in the logic to compile the source
language to the target language, or the existence of bugs in general-purpose hardware
circuits which have not been proven correct.

There is also a lot to take into consideration in designing the proof protocol itself,
the implementation of proving, and the statements to be proven. In what type of language
should the statements to be proven be written, and how should those statements together
with data satisfying them be turned into proofs?

A popular approach to answering these questions is to consider all proving problems
as cases of the problem of verifying the results of computations, and to express the
statements to be proven in the form ``program $X$ when run on input $Y$ results in $Z$.''
This approach has the advantages of generality, familiarity for software engineers, and the
existence of well studied methods to do it and well capitalized projects to do it.
However, it is not the only approach worth considering, because it does have some
major limitations. 

Consider step 3(a) in the previously outlined process for knowing the correct answer
to a question: that is to express the intentions correctly as propositions in a formal
language. Programs do not express propositions; they express computational processes.
Programs can be used to express propositions indirectly, by writing a program which
would have a certain output (say 1) only if the proposition is true. However, if one
takes this approach, it's best not to assume, but to prove \emph{a priori}\/ that
the program would have that certain output only if the proposition of interest is true.
The difficulty at this point is that proving things about programs that run on von
Neumann\footnote{or Harvard architecture} machines is typically hard, requiring
lots of time and effort of highly specialized proof engineers.

Proving things about programs that run on von Neumann machines is an unnecessary step,
if we consider that we can express propositions directly in a formal language, say a
version of predicate logic, and that we can potentially generate succinct proofs of
those propositions, potentially without using a von Neumann machine in the process
of verifying those proofs (instead using specialized, proven circuits). If we take
that idea to its logical conclusion, then we can imagine a process where we don't need
to prove facts about the behavior of a von Neumann machine in order to performs steps
3(a-b), where we check that if our base assumptions are satisfied then our system
will behave as intended.

Step 3(a) is in this scenario is the process of determining,
through a combination of cognition and testing, that we correctly expressed the
intentions in a formal language. Step 3(b) involves proving that the succinct
proving protocol is correct, in the sense that what it proves is whatever proposition
is expressed as its input. However, the implementation of the proving protocol need
not be proven correct in its entirety; what must be proven correct is the implementation
of the subset of the proving protocol which the verifier must perform. If the proving
protocol implementation is wrong, but the verifier implementation is correct up to a
negligible probability of unsoundness, then the worst that has a non-negligible probability
of happening is a failure to prove something true (but which can be proven to the
verifier's satisfaction by a correct prover), and typically that would be more acceptable than
the verifier certifying something as true which is false.

The preceding argument outlines some of the considerations that are going into the
design and implementation of OSL, the Open Specification Language, and its
compiler.\footnote{\url{https://github.com/CasperAssociation/osl}}
The Open Specification Language is a version of predicate logic, with some basic type theory,
which is designed to be powerful enough to be useful across the board in practice, and also weak enough
to be feasibly compile fairly directly into arithmetic circuits
for use with succinct proving systems such as Halo 2.\footnote{\url{https://github.com/zcash/halo2}}
As of this writing, we have an optimizing circuit compiler for OSL to Halo 2 circuits,
work required for using this compiler to generate zk-SNARK proofs is ongoing, and
six out of eight compiler stages are tested to be semantics preserving on a small
but interesting example.

Work to prove the OSL compilation pipeline correct is also ongoing.\footnote{This work began in
\url{https://github.com/Orbis-Tertius/Coq-Arithmetization} and is ongoing in
\url{https://github.com/CasperAssociation/Isabelle-Arithmetization/} as of this writing.}
This proving work is being carried out using proof assistants. Proving such complicated
theorems, and checking the proofs entirely by hand, is not feasible, which is why proof
assistants are needed to help discover the proofs and to check them.

This proposal opines that:
\begin{enumerate}
	\item it is a useful direction to continue the development and formal verification
		of OSL and its compiler;
	\item it will be useful to apply OSL to proving that questions have been answered
		correctly for the purposes of high stakes engineering environments;
	\item it is a useful direction because of the degrees of certainty that can
		be achieved by distributing, decentralizing, and diversifying the process of
		verification;
	\item this approach is different from its peers in that it envisions proving propositions
		without first expressing them as proposition-checking programs;
	\item such an approach is feasible to realize, and would bring down the price point
		of reliably producing knowledge and truths, when the highest obtainable degree
		of reliability in doing so is required.
\end{enumerate}

Just how broadly does this approach apply? What can we know about the behaviors of our systems
by building them in such a way? We can't know (without assumptions) that our systems will do
anything, since after all the Earth and everything on it may be vaporized by a giant laser,
leaving no part of our system left to exhibit any behaviors. All we can really know is
that the system won't exhibit certain behaviors, such as verifying a falsehood. So if we
divide behavioral requirements into positive (it \emph{does}\/ do something) and negative
(it \emph{does not}\/ do something), we may find that we can, up to a negligible probability
of failure, ensure that negative behavioral requirements are satisfied. Positive behavioral
requirements on a system may be impossible to ensure, as long as nuclear war remains a possibility,
but even if so, behavioral requirements can be adequately expressed as negative requirements, of the form
``the system \emph{does not}\/ behave except in the expected ways,'' together with an
availability (uptime) requirement such as is commonly found in service level agreements.
If we think about the requirements on a system in this way, with complex negative behavioral
requirements and a simple availability requirement stating that the system is doing behaviors
a certain percentage of the time, then we can begin to imagine using succinct proofs to
enforce those negative behavioral requirements and obtain a reliably behaving system
made up of potentially unreliable components, with no components assumed to be reliable.

\end{document}
